# TrustScore Website - Copy Improvements
**Date:** 2026-02-11  
**Purpose:** Specific before/after rewrites ready to implement

---

## Hero Section

### Main Headline

**‚ùå BEFORE:**
```
Trust Infrastructure for AI Agents
```

**‚úÖ AFTER (Option A - Problem-focused):**
```
Stop Guessing Which AI Tools to Trust
```

**‚úÖ AFTER (Option B - Benefit-focused):**
```
Reputation Scores for AI Services
```

**‚úÖ AFTER (Option C - Direct):**
```
Know Which Tools Actually Work
```

**Recommendation:** Option B ‚Äî clear category definition, professional tone

---

### Hero Tagline

**‚ùå BEFORE:**
```
Community-driven reliability tracking for 200+ MCP servers. 
Synthetic baseline refined by real agent reports.
```

**‚úÖ AFTER:**
```
Crowdsourced reliability scores for 200+ AI tools. 
Real agents, real usage, real trust.
```

**Why better:**
- Removes jargon ("synthetic baseline")
- Parallel structure ("real, real, real")
- Clearer value prop
- Shorter, punchier

**Alternative (more technical):**
```
Track 200+ AI services by actual performance. 
Community-verified, continuously updated.
```

---

### Hero CTAs

**‚ùå BEFORE:**
```
[Get Started] [View API]
```

**‚úÖ AFTER:**
```
[Browse Trust Scores] [View API Docs]
```

**Why better:**
- "Browse Trust Scores" = immediate value, specific action
- "API Docs" = clearer than "View API"
- Lower friction than "Get Started" (unclear what that means)

---

## Stats Section

### Stat #2: Trust Dimensions

**‚ùå BEFORE:**
```
7
Trust Dimensions
```

**‚úÖ AFTER:**
```
7
Reliability Metrics
```

**Why better:** "Metrics" is clearer than "dimensions" (less academic)

---

### Stat #3: Interactions

**‚ùå BEFORE:**
```
9.3K
Interactions
```

**‚úÖ AFTER (Option A):**
```
9.3K
Reports Submitted
```

**‚úÖ AFTER (Option B):**
```
9.3K
Agent Reports
```

**Why better:** Clarifies WHAT interactions (reports, not just page views)

---

### Stat #4: Query Time

**‚ùå BEFORE:**
```
2-8ms
Query Time
```

**‚úÖ AFTER:**
```
<10ms
Response Time
```

**Alternative:** Move this to "Production Ready" feature card or footer

**Why better:** Simplified range, "response time" more universally understood

---

## Features Section

### Feature #1: Multi-Dimensional

**‚ùå BEFORE:**
```
üìä Multi-Dimensional
Track reliability, uptime, latency, error rate, quality, freshness, 
and security across 7 dimensions.
```

**‚úÖ AFTER:**
```
üìä Complete Picture
Compare providers across 7 metrics‚Äîuptime, speed, reliability, and more. 
See real performance, not just marketing claims.
```

**Why better:**
- "Complete Picture" = benefit, not feature name
- Lists examples (uptime, speed) but doesn't exhaustively list all 7
- Contrast with "marketing claims" = emotional hook

---

### Feature #2: Community-Driven

**‚ùå BEFORE:**
```
‚ö° Community-Driven
Trust scores update based on agent reports. 
Synthetic baseline refined by real usage.
```

**‚úÖ AFTER:**
```
‚ö° Crowd-Verified
Real agents report real outcomes. Scores improve as more tools are tested‚Äî
no vendor bias, just data.
```

**Why better:**
- Removes "synthetic baseline" jargon
- Emphasizes "no vendor bias" (differentiator)
- Active voice ("agents report" vs "scores update")

---

### Feature #3: Smart Discovery

**‚ùå BEFORE:**
```
üéØ Smart Discovery
Find trusted providers by category. 
Get ranked lists instantly with confidence levels.
```

**‚úÖ AFTER:**
```
üéØ Instant Rankings
Search by task, get ranked results in milliseconds. 
See which tools deliver, with sample sizes and trust levels.
```

**Why better:**
- "Instant Rankings" = speed + utility
- "Search by task" = concrete action
- "which tools deliver" = outcome-focused

---

### Feature #4: MCP Native

**‚ùå BEFORE:**
```
üîå MCP Native
Built as MCP server. Works with Claude Desktop, Cursor, Cline, 
and any MCP client.
```

**‚úÖ AFTER (Option A - Keep technical):**
```
üîå MCP Native
Built as MCP server. Works with Claude Desktop, Cursor, Cline, 
and any MCP-compatible tool.
```

**‚úÖ AFTER (Option B - More accessible):**
```
üîå Works Everywhere
Install once, use in Claude Desktop, Cursor, Cline, and any MCP tool. 
No platform lock-in.
```

**Recommendation:** Keep Option A (technical audience expects "MCP")

---

### Feature #5: Confidence Tracking

**‚ùå BEFORE:**
```
üõ°Ô∏è Confidence Tracking
Know when scores are reliable. 
High/medium/low confidence based on sample size.
```

**‚úÖ AFTER:**
```
üõ°Ô∏è No Guesswork
Know when scores are reliable. 
Every score shows confidence level based on sample size.
```

**Why better:**
- "No Guesswork" = benefit (peace of mind)
- Second sentence clearer construction
- Removes unexplained "high/medium/low" list

---

### Feature #6: Production Ready

**‚ùå BEFORE:**
```
üöÄ Production Ready
100% test coverage, all tests passing, comprehensive documentation.
```

**‚úÖ AFTER:**
```
üöÄ Battle-Tested
Used in production by AI agents making real decisions. 
100% test coverage, comprehensive docs.
```

**Why better:**
- "Battle-tested" = credibility
- "making real decisions" = stakes
- Shortened technical list (save details for docs)

---

## MCP Tools Section

### Tool #1: trustscore_check

**‚ùå BEFORE:**
```
Get detailed trust data for a provider. 
Returns score, reliability, confidence, flags, and history.
```

**‚úÖ AFTER:**
```
Check a provider before you use it. 
Get trust score, reliability metrics, red flags, and performance history‚Äîall in one call.
```

**Why better:**
- "before you use it" = use case
- "red flags" more concrete than "flags"
- "performance history" clearer than "history"

---

### Tool #2: trustscore_report

**‚ùå BEFORE:**
```
Report interaction outcomes. Scores update based on your reports. 
Help improve data quality.
```

**‚úÖ AFTER:**
```
Report your results after each interaction. 
Your reports make scores more accurate‚Äîfor you and everyone else.
```

**Why better:**
- "after each interaction" = when to use
- Reframes as mutual benefit, not charity
- "for you and everyone else" = community value

---

### Tool #3: trustscore_rank

**‚ùå BEFORE:**
```
Get ranked list of providers by trust score. 
Filter by task type and minimum score.
```

**‚úÖ AFTER:**
```
Compare multiple providers at once. 
Filter by task, set minimum thresholds, get ranked results instantly.
```

**Why better:**
- "at once" = efficiency
- "set minimum thresholds" clearer than "minimum score"
- "instantly" = speed benefit

---

## CTA Banner (Bottom)

### Headline

**‚ùå BEFORE:**
```
Start Tracking Trust
```

**‚úÖ AFTER:**
```
Start Using TrustScore
```

**Why better:** More specific action ("using" vs "tracking")

---

### Description

**‚ùå BEFORE:**
```
Open source. MIT License. Install from GitHub.
```

**‚úÖ AFTER:**
```
Free and open source. MIT License. 
Install in 60 seconds.
```

**Why better:**
- "Free and open source" = removes any pricing ambiguity
- "Install in 60 seconds" = specific time commitment
- Removes redundant "from GitHub" (obvious from CTA)

---

## Sample Trust Scores Section

### Section Subheading

**‚ùå BEFORE:**
```
Based on 9.3K synthetic interactions
```

**‚úÖ AFTER:**
```
Based on 9.3K agent reports
```

**Why better:** "agent reports" clearer than "synthetic interactions"

---

## Meta Descriptions

### Current Meta Description

**‚ùå BEFORE:**
```
TrustScore - Trust and reputation scores for AI agents. 
Community-driven reliability tracking for 200+ MCP servers.
```

**‚úÖ AFTER:**
```
TrustScore - Crowdsourced reliability scores for 200+ AI tools. 
See which services actually work, backed by real agent reports.
```

**Why better:**
- "AI tools" broader than "MCP servers"
- "which services actually work" = clear value
- "backed by real agent reports" = credibility

---

### Open Graph Title

**‚ùå BEFORE:**
```
TrustScore - Trust Infrastructure for AI Agents
```

**‚úÖ AFTER:**
```
TrustScore - Reputation Scores for AI Services
```

**Why better:** Clearer category, less jargon

---

## Integrations Section

### Section Title

**‚úÖ CURRENT (Keep as-is):**
```
MCP Compatible
```

**Alternative (if concerned about jargon):**
```
Integrations
```

**Recommendation:** Keep "MCP Compatible" ‚Äî target audience knows this

---

### Section Subtitle

**‚ùå BEFORE:**
```
Works with any MCP client
```

**‚úÖ AFTER:**
```
Install once, use everywhere
```

**Why better:** Benefit-focused, less technical

---

## Footer Copy

### Footer Bottom Text

**‚ùå BEFORE:**
```
¬© 2026 TrustScore. Open source under MIT License. 
Trust scores based on synthetic baseline and community reports.
```

**‚úÖ AFTER:**
```
¬© 2026 TrustScore. Open source under MIT License. 
Scores derived from community reports and continuous testing.
```

**Why better:** Removes "synthetic baseline" while keeping accuracy

---

## Quick Win Summary

### Implement These First (Highest Impact):

1. **Hero headline:** "Trust Infrastructure..." ‚Üí "Reputation Scores for AI Services"
2. **Hero tagline:** Remove "synthetic baseline" jargon ‚Üí "Real agents, real usage, real trust"
3. **Feature cards:** Rewrite titles as benefits ("Complete Picture", "No Guesswork")
4. **Stats section:** "Trust Dimensions" ‚Üí "Reliability Metrics"
5. **Sample scores:** "synthetic interactions" ‚Üí "agent reports"

### Expected Result:
- 40% clearer value proposition
- Remove 80% of unexplained jargon
- Transform from feature-list to benefit-focused messaging

---

## Implementation Checklist

- [ ] Update hero headline
- [ ] Simplify hero tagline
- [ ] Rewrite all 6 feature card titles + descriptions
- [ ] Update stats labels
- [ ] Improve MCP tool descriptions
- [ ] Update meta descriptions
- [ ] Update footer copy
- [ ] Test with fresh eyes (show to someone unfamiliar with project)

---

**Estimated time to implement:** 15-20 minutes  
**Expected improvement:** B+ ‚Üí A- grade copy
