# TrustScore Website - Copy Review
**Date:** 2026-02-11  
**Standard:** Grade A professional SaaS copy (Stripe, Vercel, Anthropic level)  
**Status:** üü° Good foundation, needs refinement for excellence

---

## Executive Summary
The website copy is **functional and clear** but lacks the **punch and specificity** that makes top-tier SaaS copy compelling. The writing is technically accurate but reads somewhat generic‚Äîit describes *what* TrustScore is without making you *feel* why you need it.

**Overall Grade:** B+ (solid professional copy, not yet Grade A)

---

## 1. Hero Section

### "Trust Infrastructure for AI Agents"
**Issue:** Generic and vague  
- ‚úÖ **What works:** "Infrastructure" signals seriousness  
- ‚ùå **What doesn't:** Could describe 50 different products  
- ‚ùå **Missing:** What problem does this solve? What pain point?  

**Better alternatives:**
- "Know Which AI Tools to Trust" (benefit-first)
- "Reputation System for AI Services" (clearer category)
- "Stop Guessing. Trust with Data." (problem ‚Üí solution)

### Tagline: "Community-driven reliability tracking for 200+ MCP servers. Synthetic baseline refined by real agent reports."
**Issues:**
- ‚úÖ **What works:** Specific number (200+), mentions community
- ‚ùå **Jargon overload:** "Synthetic baseline refined" ‚Äî what does this mean to a new user?
- ‚ùå **Passive voice:** "refined by" ‚Äî weak construction
- ‚ùå **Two ideas competing:** Community-driven OR synthetic baseline? Pick one for hero, explain other later.

**Recommended rewrite:**
> "Crowdsourced reliability scores for 200+ AI tools. Real agents, real usage, real trust."

### CTAs: "Get Started" / "View API"
- ‚úÖ **Clear and standard** ‚Äî no issues here
- **Minor improvement:** Consider "Browse Scores" instead of "View API" (less technical, more benefit-focused)

---

## 2. Stats Section

### Numbers: 202 / 7 / 9.3K / 2-8ms
**Issues:**
- ‚úÖ **Specific numbers good** (not rounded)
- ‚ùå **"7 Trust Dimensions"** ‚Äî what does this mean? Confusing without context
- ‚ùå **"9.3K Interactions"** ‚Äî is this impressive? Needs context ("per day"? "total"?)
- ‚ö†Ô∏è **"2-8ms Query Time"** ‚Äî excellent technical detail BUT most users don't care; save for docs

### Labels: "SERVERS TRACKED" vs alternatives
**Current labels:**
- Servers Tracked ‚úÖ
- Trust Dimensions ‚ùå (vague)
- Interactions ‚ö†Ô∏è (needs qualifier)
- Query Time ‚ö†Ô∏è (too technical for hero area)

**Recommended rewrites:**
- ~~"Trust Dimensions"~~ ‚Üí **"Reliability Metrics"**
- ~~"Interactions"~~ ‚Üí **"9.3K Reports Submitted"** or **"Daily Updates"**
- ~~"Query Time"~~ ‚Üí Move to "Production Ready" section or footer

---

## 3. Features Section

### Overall Assessment
- ‚úÖ **Clear structure** ‚Äî each card has icon, title, description
- ‚ùå **Feature-focused, not benefit-focused** ‚Äî tells what it does, not why I care
- ‚ùå **Some descriptions are too technical** ‚Äî "Synthetic baseline refined by real usage" appears again

### Card-by-Card Review:

#### üìä Multi-Dimensional
**Current:** "Track reliability, uptime, latency, error rate, quality, freshness, and security across 7 dimensions."

**Issues:**
- Lists features, not benefits
- "7 dimensions" repeated from stats (redundant)

**Rewrite:**
> "Compare providers across 7 metrics‚Äîuptime, speed, reliability, and more. See the full picture, not just marketing claims."

---

#### ‚ö° Community-Driven
**Current:** "Trust scores update based on agent reports. Synthetic baseline refined by real usage."

**Issues:**
- "Synthetic baseline refined" ‚Äî still confusing jargon
- Doesn't explain WHY community-driven matters

**Rewrite:**
> "Real agents report real outcomes. Scores improve as more tools are tested‚Äîno vendor bias, just data."

---

#### üéØ Smart Discovery
**Current:** "Find trusted providers by category. Get ranked lists instantly with confidence levels."

**Issues:**
- Too generic ("Smart Discovery" ‚Äî every SaaS claims this)
- "confidence levels" ‚Äî vague

**Rewrite:**
> "Search by task, get ranked results in milliseconds. See which tools actually deliver, with sample sizes and confidence scores."

---

#### üîå MCP Native
**Current:** "Built as MCP server. Works with Claude Desktop, Cursor, Cline, and any MCP client."

**Issues:**
- ‚úÖ **Actually quite good** ‚Äî specific integrations
- ‚ö†Ô∏è Minor: "MCP Native" assumes you know what MCP is

**Rewrite (optional):**
> "Works everywhere you work. Claude Desktop, Cursor, Cline‚Äîinstall once, use anywhere."

---

#### üõ°Ô∏è Confidence Tracking
**Current:** "Know when scores are reliable. High/medium/low confidence based on sample size."

**Issues:**
- Title is feature-focused ("Confidence Tracking"), not benefit
- Description is clear ‚úÖ

**Rewrite title:**
> "üõ°Ô∏è No Guesswork"

---

#### üöÄ Production Ready
**Current:** "100% test coverage, all tests passing, comprehensive documentation."

**Issues:**
- Too engineering-focused (good for footer, not hero area)
- Doesn't communicate user benefit

**Rewrite:**
> "Battle-tested and documented. Used in production by AI agents making real decisions."

---

## 4. MCP Tools Section

### Overall Assessment
- ‚úÖ **Code examples excellent** ‚Äî clear, concise, realistic
- ‚úÖ **Tool names descriptive** ‚Äî `trustscore_check`, `trustscore_report`, `trustscore_rank`
- ‚ùå **Card descriptions a bit technical** ‚Äî assumes MCP familiarity

### Card-by-Card Review:

#### `trustscore_check`
**Current:** "Get detailed trust data for a provider. Returns score, reliability, confidence, flags, and history."

**Issue:** Lists return values (good) but doesn't explain use case

**Rewrite:**
> "Check a provider before you use it. Get trust score, reliability metrics, red flags, and historical performance‚Äîall in one call."

---

#### `trustscore_report`
**Current:** "Report interaction outcomes. Scores update based on your reports. Help improve data quality."

**Issue:** "Help improve data quality" sounds like charity work, not a feature

**Rewrite:**
> "Report your results. Every outcome you share makes the scores more accurate‚Äîfor you and everyone else."

---

#### `trustscore_rank`
**Current:** "Get ranked list of providers by trust score. Filter by task type and minimum score."

**Issue:** Minor‚Äîvery clear, could be punchier

**Rewrite:**
> "Compare multiple providers at once. Filter by task, set minimum thresholds, get ranked results instantly."

---

## 5. Overall Copy Issues

### Spelling & Grammar
‚úÖ **Perfect** ‚Äî no errors found

### Tone Consistency
‚ö†Ô∏è **Inconsistent mix:**
- Hero section: Corporate/technical ("Trust Infrastructure")
- Features: Mix of friendly ("Smart Discovery") and technical ("MCP Native")
- API section: Developer-focused (appropriate)

**Recommendation:** Choose a consistent voice:
- **Option A:** Technical/precise (like Anthropic) ‚Äî "Reliability oracle for AI agents"
- **Option B:** Clear/accessible (like Stripe) ‚Äî "Trust scores for the tools your agents use"

### Jargon Audit
‚ùå **Too much unexplained jargon:**
- "Synthetic baseline" (appears 3x, never explained)
- "MCP server" (never explained before first use)
- "Trust dimensions" (vague, academic)
- "Confidence levels" (used without context)

**Fix:** Either explain on first use OR replace with plain language

---

### CTA Effectiveness

**Current CTAs:**
1. Hero: "Get Started" / "View API" ‚úÖ Clear
2. Bottom: "Get Started" / "Documentation" ‚úÖ Clear

**Issue:** No urgency, no specificity

**Improvement ideas:**
- "Browse 200+ Trust Scores" (specific action)
- "Try the API" (lower friction than "Get Started")
- "See Live Scores" (immediate value)

---

## 6. Missing Elements

### What's NOT on the page that should be:
1. **Social proof** ‚Äî "Used by X agents" or testimonials
2. **Problem statement** ‚Äî What happens WITHOUT TrustScore? (wasted time, broken tools, uncertainty)
3. **Comparison** ‚Äî How is this different from just reading GitHub stars?
4. **Use case examples** ‚Äî "When agent needs weather data, TrustScore ranks 12 providers by reliability"

---

## 7. Redundant Copy

See `REDUNDANT-TEXT.md` for removal suggestions.

---

## Priority Fixes (High ‚Üí Low)

### üî¥ High Priority
1. Replace "Trust Infrastructure for AI Agents" with benefit-driven headline
2. Simplify hero tagline (remove "synthetic baseline refined")
3. Explain or remove "synthetic baseline" jargon
4. Rewrite feature cards to emphasize benefits over features

### üü° Medium Priority
5. Add context to stats ("9.3K interactions" ‚Üí "9.3K reports submitted")
6. Remove or relocate "2-8ms query time" stat (too technical)
7. Improve feature card titles (benefit-focused)
8. Add one sentence explaining what MCP is

### üü¢ Low Priority
9. Make CTAs more specific ("Browse Scores", "Try API")
10. Add social proof or usage examples
11. Consistent tone throughout (pick technical OR accessible)

---

## Conclusion

**Strengths:**
- Technically accurate ‚úÖ
- Well-structured ‚úÖ
- Clear hierarchy ‚úÖ
- Code examples excellent ‚úÖ

**Weaknesses:**
- Generic messaging (could be any B2B SaaS)
- Jargon not explained ("synthetic baseline")
- Feature-focused, not benefit-focused
- Missing emotional hook or problem statement

**To reach Grade A:** Lead with a problem, speak in benefits, lose the jargon.

---

**Next Steps:**
1. Read `COPY-IMPROVEMENTS.md` for specific rewrites
2. Read `REDUNDANT-TEXT.md` for deletion candidates
3. Implement high-priority fixes first
4. A/B test new hero headline
